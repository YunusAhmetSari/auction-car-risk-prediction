{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Bibliotheken"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eigene Funktionen\n",
        "from core.data import clean_data, drop_columns, engineer_features, load_competition_from_kaggle, memory_data, TopNCategoriesTransformer\n",
        "\n",
        "# Datenmanipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import joblib\n",
        "import os\n",
        "\n",
        "# Visualisierung\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, f1_score, precision_score, recall_score\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.inspection import PartialDependenceDisplay\n",
        "\n",
        "# Imbalanced-learn\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Hyperparameter Optimierung\n",
        "from skopt import BayesSearchCV\n",
        "from skopt.space import Integer, Real\n",
        "\n",
        "# Warnungen unterdrücken\n",
        "import warnings\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action=\"ignore\", category=DataConversionWarning)\n",
        "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# User Input"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verzeichnisse\n",
        "SAVE_DIR = \"../data/processed/\"\n",
        "MODEL_FILENAME = \"best_model_pipeline.pkl\"\n",
        "MODEL_PATH = os.path.join(SAVE_DIR, MODEL_FILENAME)\n",
        "\n",
        "# Hyperparameter-Optimierung durchführen\n",
        "run_bayes_search = 0 # 1 -> BayesSearchCV, 0 -> Nicht durchführen"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Gather Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daten herunterladen (Kaggle Competition)\n",
        "competition_name = \"DontGetKicked\"\n",
        "destination = \"../data/raw\"\n",
        "\n",
        "files = load_competition_from_kaggle(\n",
        "    competition_name=competition_name,\n",
        "    destination=destination,\n",
        ")\n",
        "\n",
        "# Trainingsdatei finden\n",
        "train_file = [f for f in files if \"training\" in f.lower()][0]\n",
        "\n",
        "# Einlesen der Daten\n",
        "df = pd.read_csv(\"/\".join([destination, competition_name, train_file]))\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explorative Datenanalayse (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Understand Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Erste Übersicht über die Daten (Dimensions, Beschreibung, Duplikate)\n",
        "display(\n",
        "    \"Shape\",\n",
        "    df.shape,\n",
        "    \"Description\",\n",
        "    df.describe().round(2).T,\n",
        "    \"Duplicates\",\n",
        "    df.duplicated().sum(),\n",
        ")\n",
        "\n",
        "# Übersicht über die Spalten (Datentypen, fehlende Werte, eindeutige Werte, Beispielwerte)\n",
        "pd.DataFrame(\n",
        "    {\n",
        "        \"Data Types\": df.dtypes,\n",
        "        \"Missing Values\": df.isnull().sum(),\n",
        "        \"Unique Values\": df.nunique(),\n",
        "        \"Sample Values\": [df[col].sample(3, random_state=42).tolist() for col in df.columns],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outliers Detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kategoriale Features: Verteilung nach Zielvariable\n",
        "categorical_features = [\n",
        "    \"Auction\",\n",
        "    \"Transmission\",\n",
        "    \"WheelTypeID\",\n",
        "    \"WheelType\",\n",
        "    \"Nationality\",\n",
        "    \"TopThreeAmericanName\",\n",
        "    \"PRIMEUNIT\",\n",
        "    \"AUCGUART\",\n",
        "    \"IsOnlineSale\",\n",
        "]\n",
        "\n",
        "for categorical_feature in categorical_features:\n",
        "    # Füllen der fehlenden Werte mit \"Missing\"\n",
        "    df_col = df[categorical_feature].fillna(\"Missing\")\n",
        "\n",
        "    # Kreuztabelle\n",
        "    print(f\"\\n=== {categorical_feature} ===\")\n",
        "    print(pd.crosstab(df_col, df[\"IsBadBuy\"]))\n",
        "\n",
        "    # Visualisierung\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    palette = [\"#009292\", \"#074650\"]\n",
        "    sns.countplot(\n",
        "        x=df_col,\n",
        "        data=df,\n",
        "        hue=\"IsBadBuy\",\n",
        "        stat=\"proportion\",\n",
        "        order=df_col.value_counts().index,\n",
        "        palette=palette,\n",
        "    )\n",
        "    plt.title(f\"Distribution of {categorical_feature}\")\n",
        "    plt.xlabel(None)\n",
        "    plt.ylabel(\"Proportion\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hochkardinalen kategoriale Features: Verteilung nach Zielvariable\n",
        "high_cardinality_categorical_features = [\n",
        "    \"Make\", \n",
        "    \"Color\", \n",
        "    \"Size\", \n",
        "    \"VNST\", \n",
        "    \"Model\", \n",
        "    \"Trim\", \n",
        "    \"SubModel\", \n",
        "    \"BYRNO\", \n",
        "    \"VNZIP1\"]\n",
        "top_n = 10\n",
        "\n",
        "for categorical_feature in high_cardinality_categorical_features:\n",
        "    # Füllen der fehlenden Werte mit \"Missing\"\n",
        "    df_col = df[categorical_feature].astype(\"str\").fillna(\"Missing\")\n",
        "\n",
        "    # Top-N-Kategorien behalten, Rest als \"Other\" zusammenfassen\n",
        "    top_n_categories = df_col.value_counts().head(top_n).index\n",
        "    df_col = df_col.where(df_col.isin(top_n_categories), other=\"Other\")\n",
        "\n",
        "    # Kreuztabelle\n",
        "    print(pd.crosstab(df_col, df[\"IsBadBuy\"]))\n",
        "\n",
        "    # Visualisierung\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    palette = [\"#009292\", \"#074650\"]\n",
        "    sns.countplot(\n",
        "        x=df_col,\n",
        "        data=df,\n",
        "        hue=\"IsBadBuy\",\n",
        "        stat=\"proportion\",\n",
        "        order=df_col.value_counts().index,\n",
        "        palette=palette,\n",
        "    )\n",
        "    plt.title(f\"Distribution of {categorical_feature} (Top {top_n} + Other)\")\n",
        "    plt.xlabel(None)\n",
        "    plt.ylabel(\"Proportion\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numerische Features: Verteilung nach Zielvariable\n",
        "numerical_features = [\n",
        "    \"MMRAcquisitionAuctionAveragePrice\",\n",
        "    \"MMRAcquisitionAuctionCleanPrice\",\n",
        "    \"MMRAcquisitionRetailAveragePrice\",\n",
        "    \"MMRAcquisitonRetailCleanPrice\",\n",
        "    \"MMRCurrentAuctionAveragePrice\",\n",
        "    \"MMRCurrentAuctionCleanPrice\",\n",
        "    \"MMRCurrentRetailAveragePrice\",\n",
        "    \"MMRCurrentRetailCleanPrice\",\n",
        "    \"VehBCost\",\n",
        "    \"WarrantyCost\",\n",
        "    \"VehOdo\",\n",
        "    \"VehicleAge\",\n",
        "]\n",
        "\n",
        "for numerical_feature in numerical_features:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
        "    palette = [\"#009292\", \"#074650\"]\n",
        "\n",
        "    sns.histplot(\n",
        "        x=df[numerical_feature], \n",
        "        kde=True, ax=axes[0], \n",
        "        color=palette[0]\n",
        "    )\n",
        "    sns.boxplot(\n",
        "        data=df, \n",
        "        x=\"IsBadBuy\", \n",
        "        y=numerical_feature, \n",
        "        hue=\"IsBadBuy\", \n",
        "        ax=axes[1], \n",
        "        palette=palette\n",
        "    )\n",
        "    axes[0].set_title(f\"Histogram of {numerical_feature}\")\n",
        "    axes[1].set_title(f\"Boxplot of {numerical_feature}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Korrelationen\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"viridis\", center=0, fmt=\".2f\")\n",
        "plt.title(\"Korrelationsmatrix (numerische Features)\")\n",
        "plt.show()\n",
        "\n",
        "# Zielverteilung\n",
        "print(\"Verteilung des Targets (gesamt):\")\n",
        "print(df[\"IsBadBuy\"].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train-Test-Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Test-Split (stratifiziert wegen Klassenungleichgewicht)\n",
        "target_col = \"IsBadBuy\"\n",
        "features = df.drop(columns=target_col)\n",
        "target = df[target_col]\n",
        "\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "    features,\n",
        "    target,\n",
        "    random_state=42,\n",
        "    test_size=0.1,\n",
        "    stratify=target\n",
        ")\n",
        "\n",
        "# Überprüfung des Train-Test-Splits\n",
        "print(\"Dimensionen der Trainingsdaten (Features):\", features_train.shape)\n",
        "print(\"Dimensionen der Testdaten (Features):\", features_test.shape)\n",
        "print(\"\\nVerteilung des Targets im Trainings-Set:\")\n",
        "print(target_train.value_counts(normalize=True))\n",
        "print(\"\\nVerteilung des Targets im Test-Set:\")\n",
        "print(target_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Datatype Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean data für die Modell-Pipeline vorbereiten\n",
        "clean_step = FunctionTransformer(\n",
        "    clean_data, \n",
        "    validate=False\n",
        ")\n",
        "\n",
        "# Bereinigung und Datentypkonvertierung der Trainings- undd Testdaten\n",
        "features_train_clean = clean_data(features_train)\n",
        "\n",
        "# Vergleich Datentypkonvertierung vorher und nachher\n",
        "dtypes_before = features_train.dtypes\n",
        "dtypes_after = features_train_clean.dtypes\n",
        "dtype_comparison = pd.DataFrame({\"dtypes_before\": dtypes_before, \"dtypes_after\": dtypes_after})\n",
        "display(dtype_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Imputation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering auf bereinigte Daten anwenden\n",
        "features_train_engineered = engineer_features(features_train_clean)\n",
        "\n",
        "# Features trennen in numerische und kategoriale Spalten\n",
        "numerical_features = features_train_engineered.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = features_train_engineered.select_dtypes(include=\"object\").columns.tolist()\n",
        "\n",
        "# Kategorische Feature-Gruppen definieren\n",
        "high_cardinality_categorical_features = [\n",
        "    \"Make\",\n",
        "    \"Color\",\n",
        "    \"Size\",\n",
        "    \"VNST\",\n",
        "    \"Model\",\n",
        "    \"Trim\",\n",
        "    \"SubModel\",\n",
        "    \"BYRNO\",\n",
        "    \"VNZIP1\",\n",
        "]\n",
        "\n",
        "high_card_features = [\n",
        "    col for col in high_cardinality_categorical_features if col in categorical_features\n",
        "]\n",
        "low_card_features = [col for col in categorical_features if col not in high_card_features]\n",
        "\n",
        "mode_features = [\"Transmission\", \"IsOnlineSale\"]\n",
        "low_card_mode_features = [col for col in low_card_features if col in mode_features]\n",
        "low_card_missing_features = [\n",
        "    col for col in low_card_features if col not in low_card_mode_features\n",
        "]\n",
        "\n",
        "# Imputation testen (ohne Encoding), um fehlende Werte zu kontrollieren\n",
        "imputer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), numerical_features),\n",
        "        (\"cat_mode\", SimpleImputer(strategy=\"most_frequent\"), low_card_mode_features),\n",
        "        (\n",
        "            \"cat_missing\",\n",
        "            SimpleImputer(strategy=\"constant\", fill_value=\"Missing\"),\n",
        "            low_card_missing_features + high_card_features,\n",
        "        ),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False,\n",
        ").set_output(transform=\"pandas\")\n",
        "\n",
        "features_train_imputed = imputer.fit_transform(features_train_engineered)\n",
        "\n",
        "# Vergleich fehlende Werte vor und nach der Imputation\n",
        "missing_values_before = features_train_engineered.isna().sum()\n",
        "missing_values_after = features_train_imputed.isna().sum()\n",
        "missing_values_comparison = pd.DataFrame(\n",
        "    {\n",
        "        \"missing_values_before\": missing_values_before,\n",
        "        \"missing_values_after\": missing_values_after,\n",
        "    }\n",
        ")\n",
        "display(missing_values_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering für die Modell-Pipeline vorbereiten\n",
        "feature_engineering_step = FunctionTransformer(\n",
        "    engineer_features,\n",
        "    validate=False\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Feature Selection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature selection für die Modell-Pipeline vorbereiten\n",
        "feature_drop = [\"PurchDate\", \"VehYear\", \"WheelTypeID\", \"VNZIP1\", # redundant\n",
        "                \"BYRNO\", # käufer id\n",
        "                \"IsOnlineSale\", \"Transmission\", \"Nationality\" # feature importance / eda\n",
        "               ]\n",
        "\n",
        "feature_selection_step = FunctionTransformer(\n",
        "    drop_columns, \n",
        "    kw_args={'cols_to_drop': feature_drop}, \n",
        "    validate=False\n",
        ")\n",
        "\n",
        "final_num_features = [col for col in numerical_features if col not in feature_drop]\n",
        "final_cat_features = [col for col in categorical_features if col not in feature_drop]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Scaling + Dimensonality Reduction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA-Features definieren und restliche numerische Features bestimmen\n",
        "pca_cols = [\n",
        "    \"MMRAcquisitionAuctionAveragePrice\",\n",
        "    \"MMRAcquisitionAuctionCleanPrice\",\n",
        "    \"MMRAcquisitionRetailAveragePrice\",\n",
        "    \"MMRAcquisitonRetailCleanPrice\",\n",
        "    \"MMRCurrentAuctionAveragePrice\",\n",
        "    \"MMRCurrentAuctionCleanPrice\",\n",
        "    \"MMRCurrentRetailAveragePrice\",\n",
        "    \"MMRCurrentRetailCleanPrice\",\n",
        "]\n",
        "\n",
        "pca_numeric_features = [col for col in pca_cols if col in final_num_features]\n",
        "non_pca_numeric_features = [col for col in final_num_features if col not in pca_numeric_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**OHE Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kategorische Feature-Gruppen definieren\n",
        "high_cardinality_categorical_features = [\n",
        "    \"Make\",\n",
        "    \"Color\",\n",
        "    \"Size\",\n",
        "    \"VNST\",\n",
        "    \"Model\",\n",
        "    \"Trim\",\n",
        "    \"SubModel\",\n",
        "    \"BYRNO\",\n",
        "    \"VNZIP1\",\n",
        "]\n",
        "\n",
        "high_card_features = [\n",
        "    col for col in high_cardinality_categorical_features if col in final_cat_features\n",
        "]\n",
        "low_card_features = [col for col in final_cat_features if col not in high_card_features]\n",
        "\n",
        "mode_features = [\"Transmission\", \"IsOnlineSale\"]\n",
        "low_card_mode_features = [col for col in low_card_features if col in mode_features]\n",
        "low_card_missing_features = [\n",
        "    col for col in low_card_features if col not in low_card_mode_features\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Arbeitsspeicher Optimierung**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Arbeitsspeicher Optimierung für die Modell-Pipeline vorbereiten\n",
        "def apply_memory_data(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Wrapper to make memory_data usable in sklearn pipelines.\"\"\"\n",
        "    return memory_data(X)\n",
        "\n",
        "memory_step = FunctionTransformer(apply_memory_data, validate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Baseline Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing- und Modell-Pipeline (inkl. PCA und Resampling)\n",
        "numeric_transformer_non_pca = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "numeric_transformer_pca = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"pca\", PCA(n_components=0.95, random_state=42)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer_low_card_mode = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer_mode\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder_mode\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer_low_card_missing = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer_missing\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
        "        (\"encoder_missing\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer_high_card = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
        "        (\"top_n\", TopNCategoriesTransformer(top_n=19)),\n",
        "        (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer_non_pca, non_pca_numeric_features),\n",
        "        (\"pca\", numeric_transformer_pca, pca_numeric_features),\n",
        "        (\"cat_high_card\", categorical_transformer_high_card, high_card_features),\n",
        "        (\"cat_mode\", categorical_transformer_low_card_mode, low_card_mode_features),\n",
        "        (\"cat_missing\", categorical_transformer_low_card_missing, low_card_missing_features),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False,\n",
        ").set_output(transform=\"pandas\")\n",
        "\n",
        "model_pipeline = ImbPipeline(\n",
        "    steps=[\n",
        "        (\"clean\", clean_step),\n",
        "        (\"feature_engineering\", feature_engineering_step),\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"memory\", memory_step),\n",
        "        (\"sampler\", RandomUnderSampler(random_state=42)),\n",
        "        (\"model\", RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Modell Evaluierung**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mehrere Modelle testen\n",
        "models = {\n",
        "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=42),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"KNeighbors\": KNeighborsClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model_pipeline = ImbPipeline(\n",
        "        steps=[\n",
        "            (\"clean\", clean_step),\n",
        "            (\"feature_engineering\", feature_engineering_step),\n",
        "            (\"feature_selection\", feature_selection_step),\n",
        "            (\"preprocess\", preprocessor),\n",
        "            (\"sampler\", RandomUnderSampler(random_state=42)),\n",
        "            (\"model\", model),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model_pipeline.fit(features_train, target_train)\n",
        "    target_test_pred = model_pipeline.predict(features_test)\n",
        "\n",
        "    results.append(\n",
        "        {\n",
        "            \"Model\": model_name,\n",
        "            \"Precision\": precision_score(target_test, target_test_pred),\n",
        "            \"Recall\": recall_score(target_test, target_test_pred),\n",
        "            \"F1\": f1_score(target_test, target_test_pred),\n",
        "        }\n",
        "    )\n",
        "\n",
        "results_df = pd.DataFrame(results).sort_values(by=\"Recall\", ascending=False)\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Optimierung der Hyperparameter mit BayesSearchCV**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "if run_bayes_search == 1:\n",
        "    # Pipeline\n",
        "    pipeline_final = ImbPipeline(\n",
        "            steps=[\n",
        "                (\"clean\", clean_step),\n",
        "                (\"feature_engineering\", feature_engineering_step),\n",
        "                (\"feature_selection\", feature_selection_step),\n",
        "                (\"preprocess\", preprocessor),\n",
        "                (\"sampler\", RandomUnderSampler(random_state=42)),\n",
        "                (\"model\", XGBClassifier(random_state=42)),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "    # Suchraum für die Hyperparameter\n",
        "    search_space_xgb = {\n",
        "        # Anzahl der Bäume\n",
        "        \"model__n_estimators\": Integer(50, 500),\n",
        "        # Maximale Tiefe\n",
        "        \"model__max_depth\": Integer(3, 15),\n",
        "        # Lernrate\n",
        "        \"model__learning_rate\": Real(0.01, 0.3, prior='log-uniform'),\n",
        "        # Instanz-Gewichte\n",
        "        \"model__min_child_weight\": Integer(1, 10),\n",
        "        # Subsample Ratio der Trainingsinstanzen (gegen Overfitting)\n",
        "        \"model__subsample\": Real(0.5, 1.0),\n",
        "        # Subsample Ratio der Spalten pro Baum\n",
        "        \"model__colsample_bytree\": Real(0.5, 1.0),\n",
        "        # Minimum loss reduction (Regularisierung)\n",
        "        \"model__gamma\": Real(0, 5)\n",
        "    }\n",
        "\n",
        "    # Stratified K-Fold Cross-Validation\n",
        "    cv_stratified = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "    # Bayesianische Optimierung mit BayesSearchCV\n",
        "    bayes_model_final = BayesSearchCV(estimator=pipeline_final,\n",
        "                                search_spaces=search_space_xgb,\n",
        "                                n_iter=100,\n",
        "                                scoring=\"f1\",\n",
        "                                cv=cv_stratified,\n",
        "                                n_jobs=-1,\n",
        "                                random_state=42)\n",
        "\n",
        "    # Fitting auf Trainingsdaten\n",
        "    bayes_model_final.fit(features_train, target_train)\n",
        "\n",
        "    # Bestes Modell\n",
        "    best_model_final = bayes_model_final.best_estimator_\n",
        "\n",
        "    # Vorhersage auf Testdaten\n",
        "    target_test_pred_final = best_model_final.predict(features_test)\n",
        "\n",
        "    print(f\"Optimierte Hyperparameter für RandomForestClassifier :\\n{bayes_model_final.best_params_}\")\n",
        "    print(\"\\nClassification report\")\n",
        "    report_final = classification_report(target_test, target_test_pred_final)\n",
        "    print(report_final)\n",
        "\n",
        "    # Speichern des besten Modells (fertige Pipeline)\n",
        "    joblib.dump(best_model_final, MODEL_PATH)\n",
        "    print(\"Modell gespeichert.\")\n",
        "\n",
        "else:\n",
        "    best_model_final = joblib.load(MODEL_PATH) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Anwendung des Modells auf Zieldaten"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Testdatei finden\n",
        "test_file = [f for f in files if \"test\" in f.lower()][0]\n",
        "\n",
        "# Einlesen der Daten\n",
        "df_aim = pd.read_csv(\"/\".join([destination, competition_name, test_file]))\n",
        "print(df_aim.shape)\n",
        "\n",
        "# Vorhersage\n",
        "df_aim[\"IsBadBuy\"] = best_model_final.predict(df_aim)\n",
        "\n",
        "print(\"\\nVerteilung des Targets 'IsBadBuy':\\n\")\n",
        "print(df_aim[\"IsBadBuy\"].value_counts(),\"\\n\")\n",
        "print(df_aim[\"IsBadBuy\"].value_counts(normalize=True),\"\\n\")\n",
        "display(df_aim.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Importance\n",
        "final_model = best_model_final.named_steps[\"model\"]\n",
        "final_preprocessor = best_model_final.named_steps[\"preprocess\"]\n",
        "\n",
        "feature_names = final_preprocessor.get_feature_names_out()\n",
        "\n",
        "if hasattr(final_model, \"feature_importances_\"):\n",
        "    importances = final_model.feature_importances_\n",
        "    fi_df = (\n",
        "        pd.DataFrame({\"feature\": feature_names, \"importance\": importances})\n",
        "        .sort_values(\"importance\", ascending=False)\n",
        "        .head(20)\n",
        "    )\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.barplot(data=fi_df, x=\"importance\", y=\"feature\", palette = [\"#009292\", \"#074650\"])\n",
        "    plt.title(\"Top 20 Feature Importances\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    display(fi_df)\n",
        "else:\n",
        "    print(\"Das finale Modell unterstützt keine feature_importances_.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PDP für VehicleAge und VehOdo\n",
        "features_pdp = [\"VehicleAge\", \"VehOdo\"]\n",
        "sample = features_train.sample(5000, random_state=42)\n",
        "\n",
        "fig, ax = plt.subplots(2, 1, figsize=(16, 10))\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(\n",
        "    best_model_final,\n",
        "    X=sample,\n",
        "    features=[\"VehicleAge\"],\n",
        "    ax=ax[0],\n",
        "    kind=\"both\",\n",
        "    ice_lines_kw={\"color\": \"#009292\", \"alpha\": 0.15},\n",
        "    pd_line_kw={\"color\": \"#074650\", \"linewidth\": 2.0},\n",
        ")\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(\n",
        "    best_model_final,\n",
        "    X=sample,\n",
        "    features=[\"VehOdo\"],\n",
        "    ax=ax[1],\n",
        "    kind=\"both\",\n",
        "    ice_lines_kw={\"color\": \"#009292\", \"alpha\": 0.15},\n",
        "    pd_line_kw={\"color\": \"#074650\", \"linewidth\": 2.0},\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PDP Interaktion zwischen VehicleAge und VehOdo\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "\n",
        "PartialDependenceDisplay.from_estimator(\n",
        "    best_model_final,\n",
        "    X=sample,\n",
        "    features=[(\"VehicleAge\", \"VehOdo\")],\n",
        "    ax=ax,\n",
        "    kind = \"average\",\n",
        "    subsample = 1\n",
        ")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
