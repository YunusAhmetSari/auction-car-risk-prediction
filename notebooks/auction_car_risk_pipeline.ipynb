{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Eigene Funktionen\n",
        "from core.data import clean_data, engineer_features, load_competition_from_kaggle, memory_data, TopNCategoriesTransformer\n",
        "\n",
        "# Datenmanipulation\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Visualisierung\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, FunctionTransformer, StandardScaler\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier \n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "# Imbalanced-learn\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "# Warnungen unterdrücken\n",
        "import warnings\n",
        "from sklearn.exceptions import DataConversionWarning\n",
        "warnings.filterwarnings(action=\"ignore\", category=DataConversionWarning)\n",
        "warnings.filterwarnings(action=\"ignore\", category=FutureWarning)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Gather Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Daten herunterladen (Kaggle Competition)\n",
        "competition_name = \"DontGetKicked\"\n",
        "destination = \"../data/raw\"\n",
        "\n",
        "files = load_competition_from_kaggle(\n",
        "    competition_name=competition_name,\n",
        "    destination=destination,\n",
        ")\n",
        "\n",
        "# Trainingsdatei finden\n",
        "train_file = [f for f in files if \"training\" in f.lower()][0]\n",
        "\n",
        "# Einlesen der Daten\n",
        "df = pd.read_csv(\"/\".join([destination, competition_name, train_file]))\n",
        "print(df.shape)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Explorative Datenanalayse (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Understand Data**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Erste Übersicht über die Daten (Dimensions, Beschreibung, Duplikate)\n",
        "display(\n",
        "    \"Shape\",\n",
        "    df.shape,\n",
        "    \"Description\",\n",
        "    df.describe().round(2).T,\n",
        "    \"Duplicates\",\n",
        "    df.duplicated().sum(),\n",
        ")\n",
        "\n",
        "# Übersicht über die Spalten (Datentypen, fehlende Werte, eindeutige Werte, Beispielwerte)\n",
        "pd.DataFrame(\n",
        "    {\n",
        "        \"Data Types\": df.dtypes,\n",
        "        \"Missing Values\": df.isnull().sum(),\n",
        "        \"Unique Values\": df.nunique(),\n",
        "        \"Sample Values\": [df[col].sample(3, random_state=42).tolist() for col in df.columns],\n",
        "    }\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Outliers Detection**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kategoriale Features: Verteilung nach Zielvariable\n",
        "categorical_features = [\n",
        "    \"Auction\",\n",
        "    \"Transmission\",\n",
        "    \"WheelTypeID\",\n",
        "    \"WheelType\",\n",
        "    \"Nationality\",\n",
        "    \"TopThreeAmericanName\",\n",
        "    \"PRIMEUNIT\",\n",
        "    \"AUCGUART\",\n",
        "    \"IsOnlineSale\",\n",
        "]\n",
        "\n",
        "for categorical_feature in categorical_features:\n",
        "    # Füllen der fehlenden Werte mit \"Missing\"\n",
        "    df_col = df[categorical_feature].fillna(\"Missing\")\n",
        "\n",
        "    # Kreuztabelle\n",
        "    print(f\"\\n=== {categorical_feature} ===\")\n",
        "    print(pd.crosstab(df_col, df[\"IsBadBuy\"]))\n",
        "\n",
        "    # Visualisierung\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    palette = [\"#009292\", \"#074650\"]\n",
        "    sns.countplot(\n",
        "        x=df_col,\n",
        "        data=df,\n",
        "        hue=\"IsBadBuy\",\n",
        "        stat=\"proportion\",\n",
        "        order=df_col.value_counts().index,\n",
        "        palette=palette,\n",
        "    )\n",
        "    plt.title(f\"Distribution of {categorical_feature}\")\n",
        "    plt.xlabel(None)\n",
        "    plt.ylabel(\"Proportion\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Hochkardinalen kategoriale Features: Verteilung nach Zielvariable\n",
        "high_cardinality_categorical_features = [\n",
        "    \"Make\", \n",
        "    \"Color\", \n",
        "    \"Size\", \n",
        "    \"VNST\", \n",
        "    \"Model\", \n",
        "    \"Trim\", \n",
        "    \"SubModel\", \n",
        "    \"BYRNO\", \n",
        "    \"VNZIP1\"]\n",
        "top_n = 10\n",
        "\n",
        "for categorical_feature in high_cardinality_categorical_features:\n",
        "    # Füllen der fehlenden Werte mit \"Missing\"\n",
        "    df_col = df[categorical_feature].astype(\"str\").fillna(\"Missing\")\n",
        "\n",
        "    # Top-N-Kategorien behalten, Rest als \"Other\" zusammenfassen\n",
        "    top_n_categories = df_col.value_counts().head(top_n).index\n",
        "    df_col = df_col.where(df_col.isin(top_n_categories), other=\"Other\")\n",
        "\n",
        "    # Kreuztabelle\n",
        "    print(pd.crosstab(df_col, df[\"IsBadBuy\"]))\n",
        "\n",
        "    # Visualisierung\n",
        "    plt.figure(figsize=(16, 6))\n",
        "    palette = [\"#009292\", \"#074650\"]\n",
        "    sns.countplot(\n",
        "        x=df_col,\n",
        "        data=df,\n",
        "        hue=\"IsBadBuy\",\n",
        "        stat=\"proportion\",\n",
        "        order=df_col.value_counts().index,\n",
        "        palette=palette,\n",
        "    )\n",
        "    plt.title(f\"Distribution of {categorical_feature} (Top {top_n} + Other)\")\n",
        "    plt.xlabel(None)\n",
        "    plt.ylabel(\"Proportion\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Numerische Features: Verteilung nach Zielvariable\n",
        "numerical_features = [\n",
        "    \"MMRAcquisitionAuctionAveragePrice\",\n",
        "    \"MMRAcquisitionAuctionCleanPrice\",\n",
        "    \"MMRAcquisitionRetailAveragePrice\",\n",
        "    \"MMRAcquisitonRetailCleanPrice\",\n",
        "    \"MMRCurrentAuctionAveragePrice\",\n",
        "    \"MMRCurrentAuctionCleanPrice\",\n",
        "    \"MMRCurrentRetailAveragePrice\",\n",
        "    \"MMRCurrentRetailCleanPrice\",\n",
        "    \"VehBCost\",\n",
        "    \"WarrantyCost\",\n",
        "    \"VehOdo\",\n",
        "    \"VehicleAge\",\n",
        "]\n",
        "\n",
        "for numerical_feature in numerical_features:\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(16, 4))\n",
        "    palette = [\"#009292\", \"#074650\"]\n",
        "\n",
        "    sns.histplot(\n",
        "        x=df[numerical_feature], \n",
        "        kde=True, ax=axes[0], \n",
        "        color=palette[0]\n",
        "    )\n",
        "    sns.boxplot(\n",
        "        data=df, \n",
        "        x=\"IsBadBuy\", \n",
        "        y=numerical_feature, \n",
        "        hue=\"IsBadBuy\", \n",
        "        ax=axes[1], \n",
        "        palette=palette\n",
        "    )\n",
        "    axes[0].set_title(f\"Histogram of {numerical_feature}\")\n",
        "    axes[1].set_title(f\"Boxplot of {numerical_feature}\")\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Korrelationen\n",
        "plt.figure(figsize=(12, 10))\n",
        "sns.heatmap(df.corr(numeric_only=True), annot=True, cmap=\"viridis\", center=0, fmt=\".2f\")\n",
        "plt.title(\"Korrelationsmatrix (numerische Features)\")\n",
        "plt.show()\n",
        "\n",
        "# Zielverteilung\n",
        "print(\"Verteilung des Targets (gesamt):\")\n",
        "print(df[\"IsBadBuy\"].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Split"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Train-Test-Split**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train/Test-Split (stratifiziert wegen Klassenungleichgewicht)\n",
        "target_col = \"IsBadBuy\"\n",
        "features = df.drop(columns=target_col)\n",
        "target = df[target_col]\n",
        "\n",
        "features_train, features_test, target_train, target_test = train_test_split(\n",
        "    features,\n",
        "    target,\n",
        "    random_state=42,\n",
        "    test_size=0.1,\n",
        "    stratify=target\n",
        ")\n",
        "\n",
        "# Überprüfung des Train-Test-Splits\n",
        "print(\"Dimensionen der Trainingsdaten (Features):\", features_train.shape)\n",
        "print(\"Dimensionen der Testdaten (Features):\", features_test.shape)\n",
        "print(\"\\nVerteilung des Targets im Trainings-Set:\")\n",
        "print(target_train.value_counts(normalize=True))\n",
        "print(\"\\nVerteilung des Targets im Test-Set:\")\n",
        "print(target_test.value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Data Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Datatype Transformation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Clean data für die Modell-Pipeline vorbereiten\n",
        "def apply_clean(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Wrapper to make clean_data usable in sklearn pipelines.\"\"\"\n",
        "    return clean_data(X)\n",
        "\n",
        "clean_step = FunctionTransformer(apply_clean, validate=False)\n",
        "\n",
        "# Bereinigung und Datentypkonvertierung der Trainings- undd Testdaten\n",
        "features_train_clean = apply_clean(features_train)\n",
        "\n",
        "# Vergleich Datentypkonvertierung vorher und nachher\n",
        "dtypes_before = features_train.dtypes\n",
        "dtypes_after = features_train_clean.dtypes\n",
        "dtype_comparison = pd.DataFrame({\"dtypes_before\": dtypes_before, \"dtypes_after\": dtypes_after})\n",
        "display(dtype_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Imputation**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering auf bereinigte Daten anwenden\n",
        "features_train_engineered = engineer_features(features_train_clean.copy())\n",
        "\n",
        "# Features trennen in numerische und kategoriale Spalten\n",
        "numeric_features = features_train_engineered.select_dtypes(include=np.number).columns.tolist()\n",
        "categorical_features = features_train_engineered.select_dtypes(include=\"object\").columns.tolist()\n",
        "\n",
        "# Kategorische Feature-Gruppen definieren\n",
        "high_cardinality_categorical_features = [\n",
        "    \"Make\",\n",
        "    \"Color\",\n",
        "    \"Size\",\n",
        "    \"VNST\",\n",
        "    \"Model\",\n",
        "    \"Trim\",\n",
        "    \"SubModel\",\n",
        "    \"BYRNO\",\n",
        "    \"VNZIP1\",\n",
        "]\n",
        "\n",
        "high_card_features = [\n",
        "    col for col in high_cardinality_categorical_features if col in categorical_features\n",
        "]\n",
        "low_card_features = [col for col in categorical_features if col not in high_card_features]\n",
        "\n",
        "mode_features = [\"Transmission\", \"IsOnlineSale\"]\n",
        "low_card_mode_features = [col for col in low_card_features if col in mode_features]\n",
        "low_card_missing_features = [\n",
        "    col for col in low_card_features if col not in low_card_mode_features\n",
        "]\n",
        "\n",
        "# Imputation testen (ohne Encoding), um fehlende Werte zu kontrollieren\n",
        "imputer = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", SimpleImputer(strategy=\"median\"), numeric_features),\n",
        "        (\"cat_mode\", SimpleImputer(strategy=\"most_frequent\"), low_card_mode_features),\n",
        "        (\n",
        "            \"cat_missing\",\n",
        "            SimpleImputer(strategy=\"constant\", fill_value=\"Missing\"),\n",
        "            low_card_missing_features + high_card_features,\n",
        "        ),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False,\n",
        ").set_output(transform=\"pandas\")\n",
        "\n",
        "features_train_imputed = imputer.fit_transform(features_train_engineered)\n",
        "\n",
        "# Vergleich fehlende Werte vor und nach der Imputation\n",
        "missing_values_before = features_train_engineered.isna().sum()\n",
        "missing_values_after = features_train_imputed.isna().sum()\n",
        "missing_values_comparison = pd.DataFrame(\n",
        "    {\n",
        "        \"missing_values_before\": missing_values_before,\n",
        "        \"missing_values_after\": missing_values_after,\n",
        "    }\n",
        ")\n",
        "display(missing_values_comparison)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Feature Engineering**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature Engineering für die Modell-Pipeline vorbereiten\n",
        "def apply_feature_engineering(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Wrapper to make engineer_features usable in sklearn pipelines.\"\"\"\n",
        "    return engineer_features(X)\n",
        "\n",
        "feature_engineering_step = FunctionTransformer(apply_feature_engineering, validate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Data Scaling + Dimensonality Reduction**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# PCA-Features definieren und restliche numerische Features bestimmen\n",
        "pca_cols = [\n",
        "    \"MMRAcquisitionAuctionAveragePrice\",\n",
        "    \"MMRAcquisitionAuctionCleanPrice\",\n",
        "    \"MMRAcquisitionRetailAveragePrice\",\n",
        "    \"MMRAcquisitonRetailCleanPrice\",\n",
        "    \"MMRCurrentAuctionAveragePrice\",\n",
        "    \"MMRCurrentAuctionCleanPrice\",\n",
        "    \"MMRCurrentRetailAveragePrice\",\n",
        "    \"MMRCurrentRetailCleanPrice\",\n",
        "]\n",
        "\n",
        "pca__numeric_features = [col for col in pca_cols if col in numeric_features]\n",
        "non_pca_numeric_features = [col for col in numeric_features if col not in pca__numeric_features]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**OHE Encoding**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kategorische Feature-Gruppen definieren\n",
        "high_cardinality_categorical_features = [\n",
        "    \"Make\",\n",
        "    \"Color\",\n",
        "    \"Size\",\n",
        "    \"VNST\",\n",
        "    \"Model\",\n",
        "    \"Trim\",\n",
        "    \"SubModel\",\n",
        "    \"BYRNO\",\n",
        "    \"VNZIP1\",\n",
        "]\n",
        "\n",
        "high_card_features = [\n",
        "    col for col in high_cardinality_categorical_features if col in categorical_features\n",
        "]\n",
        "low_card_features = [col for col in categorical_features if col not in high_card_features]\n",
        "\n",
        "mode_features = [\"Transmission\", \"IsOnlineSale\"]\n",
        "low_card_mode_features = [col for col in low_card_features if col in mode_features]\n",
        "low_card_missing_features = [\n",
        "    col for col in low_card_features if col not in low_card_mode_features\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Arbeitsspeicher Optimierung**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Arbeitsspeicher Optimierung für die Modell-Pipeline vorbereiten\n",
        "def apply_memory_data(X: pd.DataFrame) -> pd.DataFrame:\n",
        "    \"\"\"Wrapper to make memory_data usable in sklearn pipelines.\"\"\"\n",
        "    return memory_data(X)\n",
        "\n",
        "memory_step = FunctionTransformer(apply_memory_data, validate=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Modeling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Baseline Model**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preprocessing- und Modell-Pipeline (inkl. PCA und Resampling)\n",
        "numeric_transformer_non_pca = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "    ]\n",
        ")\n",
        "\n",
        "numeric_transformer_pca = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "        (\"scaler\", StandardScaler()),\n",
        "        (\"pca\", PCA(n_components=0.95, random_state=42)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer_low_card_mode = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer_mode\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "        (\"encoder_mode\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer_low_card_missing = Pipeline(\n",
        "    steps=[\n",
        "        (\"imputer_missing\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
        "        (\"encoder_missing\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "categorical_transformer_high_card_missing = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Missing\")),\n",
        "    (\"top_n\", TopNCategoriesTransformer(top_n=19)),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", sparse_output=False)),\n",
        "])\n",
        "\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        (\"num\", numeric_transformer_non_pca, non_pca_numeric_features),\n",
        "        (\"pca\", numeric_transformer_pca, pca__numeric_features),\n",
        "        (\"cat_high_card\", categorical_transformer_high_card_missing, high_card_features),\n",
        "        (\"cat_mode\", categorical_transformer_low_card_mode, low_card_mode_features),\n",
        "        (\"cat_missing\", categorical_transformer_low_card_missing, low_card_missing_features),\n",
        "    ],\n",
        "    remainder=\"drop\",\n",
        "    verbose_feature_names_out=False,\n",
        ").set_output(transform=\"pandas\")\n",
        "\n",
        "model_pipeline = ImbPipeline(\n",
        "    steps=[\n",
        "        (\"clean\", clean_step),\n",
        "        (\"feature_engineering\", feature_engineering_step),\n",
        "        (\"preprocess\", preprocessor),\n",
        "        (\"memory\", memory_step),\n",
        "        (\"sampler\", RandomUnderSampler(random_state=42)),\n",
        "        (\"model\", RandomForestClassifier(n_estimators=300, random_state=42, n_jobs=-1)),\n",
        "    ]\n",
        ")\n",
        "\n",
        "model_pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mehrere Modelle testen\n",
        "models = {\n",
        "    \"RandomForest\": RandomForestClassifier(random_state=42),\n",
        "    \"LogisticRegression\": LogisticRegression(random_state=42),\n",
        "    \"DecisionTree\": DecisionTreeClassifier(random_state=42),\n",
        "    \"KNeighbors\": KNeighborsClassifier(),\n",
        "    \"XGBoost\": XGBClassifier(random_state=42)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model_pipeline = ImbPipeline(\n",
        "        steps=[\n",
        "            (\"clean\", clean_step),\n",
        "            (\"feature_engineering\", feature_engineering_step),\n",
        "            (\"preprocess\", preprocessor),\n",
        "            (\"sampler\", RandomUnderSampler(random_state=42)),\n",
        "            (\"model\", model),\n",
        "        ]\n",
        "    )\n",
        "\n",
        "    model_pipeline.fit(features_train, target_train)\n",
        "    target_test_pred = model_pipeline.predict(features_test)\n",
        "    target_test_proba = model_pipeline.predict_proba(features_test)[:, 1] if hasattr(model_pipeline, \"predict_proba\") else None\n",
        "\n",
        "    roc_auc = roc_auc_score(target_test, target_test_proba) if target_test_proba is not None else np.nan\n",
        "\n",
        "    print(f\"\\n=== {model_name} ===\")\n",
        "    print(classification_report(target_test, target_test_pred))\n",
        "    if target_test_proba is not None:\n",
        "        print(\"ROC-AUC:\", roc_auc)\n",
        "\n",
        "    results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"ROC-AUC\": roc_auc,\n",
        "        \"Accuracy\": (target_test == target_test_pred).mean(),\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(results)\n",
        "display(results_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Model Interpretation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
